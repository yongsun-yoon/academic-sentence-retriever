{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vnCqTw7gq5h73ylQhYZsjQHxAgdj_4aH",
      "authorship_tag": "ABX9TyN6FPqR8w694wSZ6JBFQpdq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsun-yoon/academic-sentence-retriever/blob/main/01_register_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Register paper"
      ],
      "metadata": {
        "id": "3ISUUpwuPhnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "lAbbjvUPG7g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q PyPDF2 transformers faiss-cpu"
      ],
      "metadata": {
        "id": "bIZVTA8FQjDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import faiss\n",
        "import PyPDF2\n",
        "import sqlite3\n",
        "import easydict\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "_vWh6QpBQkPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = easydict.EasyDict(\n",
        "    basedir = '/content/drive/MyDrive/project/academic-sentence-retriever',\n",
        "    model_name = 'yongsun-yoon/bilingual-sentence-embedder-mMiniLMv2-L6-H384'\n",
        ")"
      ],
      "metadata": {
        "id": "QlHGA6YGefNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Read PDF"
      ],
      "metadata": {
        "id": "jGjEW098G8tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_newline(text):\n",
        "    text = list(text)\n",
        "    for i, c in enumerate(text):\n",
        "        if c != '\\n': continue\n",
        "        \n",
        "        if text[i-1] == '-':\n",
        "            text[i-1] = ''\n",
        "            text[i] = ''\n",
        "        else:\n",
        "            text[i] = ' '\n",
        "    text = ''.join(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_sentences(reader):\n",
        "    sentences = []\n",
        "    for page in reader.pages:\n",
        "        text = page.extract_text().strip()\n",
        "        text = replace_newline(text)\n",
        "        sentences += nltk.sent_tokenize(text)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "HOBZ4530c6l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_id = '2109.06349'\n",
        "pdf_path = f'{cfg.basedir}/papers/{arxiv_id}.pdf'\n",
        "assert os.path.exists(pdf_path)"
      ],
      "metadata": {
        "id": "MQiSZH4sPpyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = open(pdf_path, 'rb')\n",
        "reader = PyPDF2.PdfReader(pdf)\n",
        "sentences = extract_sentences(reader)\n",
        "print(len(sentences))"
      ],
      "metadata": {
        "id": "Capa8ITos8lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. SQLite"
      ],
      "metadata": {
        "id": "7tF3HqDeG-q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(f'{cfg.basedir}/data.sqlite')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# cursor.execute('DROP TABLE IF EXISTS sents')\n",
        "cursor.execute(\"CREATE TABLE IF NOT EXISTS sents (id integer PRIMARY KEY, sent text, arxiv_id text)\")"
      ],
      "metadata": {
        "id": "UdPTnEBjUqjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute('SELECT COUNT(*) FROM sents')\n",
        "rowid = cursor.fetchone()[0]\n",
        "print(rowid)"
      ],
      "metadata": {
        "id": "HKXKxmrUgl3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cursor.execute(f'SELECT COUNT(*) FROM sents WHERE arxiv_id = {arxiv_id} GROUP BY arxiv_id')\n",
        "res = cursor.fetchone()\n",
        "assert res is None"
      ],
      "metadata": {
        "id": "dNn91LlfRThq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [(rowid+i, s, arxiv_id) for i, s in enumerate(sentences)]\n",
        "cursor.executemany(\"INSERT INTO sents(id, sent, arxiv_id) VALUES(?,?,?)\", inputs)\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "CkY-s3VFe5jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. FAISS"
      ],
      "metadata": {
        "id": "kFfxaevTH2SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(token_embeddings, attention_mask):\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "def encode(model, tokenizer, sentences, batch_size=16, max_length=256):\n",
        "    embeds = []\n",
        "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
        "        batch_sentences = sentences[i:i+batch_size]\n",
        "        batch_inputs = tokenizer(batch_sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "        batch_outputs = model(**batch_inputs).last_hidden_state\n",
        "        batch_embeds = mean_pooling(batch_outputs, batch_inputs.attention_mask)\n",
        "        batch_embeds = F.normalize(batch_embeds, p=2, dim=1)\n",
        "        embeds.append(batch_embeds)\n",
        "    embeds = torch.cat(embeds, dim=0)\n",
        "    return embeds"
      ],
      "metadata": {
        "id": "hq0Av9HSH5b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f'{cfg.basedir}/model'\n",
        "if not os.path.exists(model_path):\n",
        "    !apt-get install git-lfs -y\n",
        "    !git-lfs install\n",
        "    !git clone https://huggingface.co/yongsun-yoon/bilingual-sentence-embedder-mMiniLMv2-L6-H384 \"{cfg.basedir}/model\"\n",
        "    print('clone from Huggingface')\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
        "_ = model.eval().requires_grad_(False)"
      ],
      "metadata": {
        "id": "00temWlw7ulw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeds = encode(model, tokenizer, sentences)\n",
        "embeds.shape"
      ],
      "metadata": {
        "id": "LIErlB39kHUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_path = f'{cfg.basedir}/data.faiss'\n",
        "if os.path.exists(index_path):\n",
        "    index = faiss.read_index(index_path)\n",
        "    print('load existed index.')\n",
        "else:\n",
        "    index = faiss.IndexFlatL2(embeds.shape[-1])\n",
        "    print('create new index.')"
      ],
      "metadata": {
        "id": "RrOYL7Jdjol9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.add(embeds)\n",
        "faiss.write_index(index, index_path)"
      ],
      "metadata": {
        "id": "ziGq9uTwjdxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJpqJcpqRzBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}